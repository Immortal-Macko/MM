---
title: "Упр 1 Бессмертная"
author: "Бессмертная М.М."
date: "18 02 2021"
output: word_document
---

```{r setup, include=FALSE}
library(Hmisc)
library(knitr)
library(corrplot)
knitr::opts_chunk$set(echo = FALSE)
```

# Исходные данные

```{r, warning=FALSE}
my.seed <- 3

# наблюдений всего
n.all <- 60
  # доля обучающей выборки
  train.percent <- 0.85
  # стандартное отклонение случайного шума
  res.sd <- 1
  # границы изменения X
  x.min <- 5
  x.max <- 105
  
  # фактические значения x
  set.seed(my.seed)
x <- runif(x.min, x.max, n = n.all)

  # случайный шум
  set.seed(my.seed)
res <- rnorm(mean = 0, sd = res.sd, n = n.all)

  # отбираем наблюдения в обучающую выборку
  set.seed(my.seed)
inTrain <-  sample(seq_along(x), size = train.percent*n.all)
  
  # истинная функция взаимосвязи 
  y.func <- function(x) {19 - 0.05 * x}

# для графика истинной взаимосвязи
x.line <- seq(x.min, x.max, length = n.all)
  y.line <- y.func(x.line)
  
  # фактические значения y (с шумом)
  y <- y.func(x) + res
  
  # Создаём векторы с данными для построения графиков ############################

# наблюдения на обучающей выборке
x.train <- x[inTrain]
  y.train <- y[inTrain]
  
  # наблюдения на тестовой выборке
  x.test <- x[-inTrain]
  y.test <- y[-inTrain]
  
  #  График 1: Исходные данные на график #########################################

# убираем широкие поля рисунка
par(mar = c(4, 4, 1, 1))

# наименьшие/наибольшие значения по осям
x.lim <- c(x.min, x.max)
y.lim <- c(min(y), max(y))

# наблюдения с шумом (обучающая выборка)
plot( x.train, y.train,
  col = grey(0.2), bg = grey(0.2), pch = 21,
  xlab = 'X', ylab = 'Y', 
  xlim = x.lim, ylim = y.lim, 
  cex = 1.2, cex.lab = 1.2, cex.axis = 1.2)

# наблюдения тестовой выборки
points(x.test, y.test,
  col = 'red', bg = 'red', pch = 21)

# истинная функция
lines(x.line, y.line,
  lwd = 2, lty = 2)

# легенда
legend('topleft', legend = c('обучение', 'тест', 'f(X)'),
       pch = c(16, 16, NA), 
       col = c(grey(0.2), 'red', 'black'),  
       lty = c(0, 0, 2), lwd = c(1, 1, 2), cex = 1.2)

#  Строим модель ###############################################################

# модель 2 (сплайн с df = 6)
mod <- smooth.spline(x = x.train, y = y.train, df = 6)

# модельные значения для расчёта ошибок
y.model.train <- predict(mod, data.frame(x = x.train))$y[, 1]
  y.model.test <- predict(mod, data.frame(x = x.train))$y[, 1]
  
  # считаем средний квадрат ошибки на обучающей и тестовой выборке
  MSE <- c(sum((y.train - y.model.train)^2) / length(x.train),
           sum((y.test - y.model.test)^2) / length(x.test))
  names(MSE) <- c('train', 'test')
  round(MSE, 2)

#  Цикл по степеням свободы ####################################################

# максимальное число степеней свободы для модели сплайна
max.df <- 40
# таблица для записи ошибок
tbl <- data.frame(df = 2:max.df)
# ошибки на обучающей выборке
tbl$MSE.train <- 0
# ошибки на тестовой выборке
tbl$MSE.test <- 0


for (i in 2:max.df) {
  # модель
  mod <-  smooth.spline(x = x.train, y = y.train, df = i)
    
    # модельные значения для расчёта ошибок
    y.model.train <- predict(mod, data.frame(x = x.train))$y[, 1]
    y.model.test <- predict(mod, data.frame(x = x.test))$y[, 1] 
    
    # считаем средний квадрат ошибки на обучающей и тестовой выборке
    MSE <- c(sum((y.train - y.model.train)^2) / length(x.train),
             sum((y.test - y.model.test)^2) / length(x.test))
    
    # записываем результат в таблицу
    tbl[tbl$df == i, c('MSE.train', 'MSE.test')] <- MSE
}

# первые строки таблицы
kable(head(tbl))

#  График 2: Зависимость MSE от гибкости модели ################################

plot(x = tbl$df, y = tbl$MSE.test, 
  type = 'l', col = 'red', lwd = 2,
  xlab = 'Степени свободы сплайна', ylab = 'MSE',
  ylim = c(min(tbl$MSE.train, tbl$MSE.test), 
           max(tbl$MSE.train, tbl$MSE.test)),
  cex = 1.2, cex.lab = 1.2, cex.axis = 1.2)

points(x = tbl$df, y = tbl$MSE.test,
  pch = 21, col = 'red', bg = 'red')

lines(x = tbl$df, y = tbl$MSE.train,
  col = grey(0.3), lwd = 2)
# неустранимая ошибка
abline(h = res.sd,
  lty = 2, col = grey(0.4), lwd = 2)

# степени свободы у наименьшей ошибки на тестовой выборке
min.MSE.test <- min(tbl$MSE.test)
  df.min.MSE.test <- tbl[tbl$MSE.test == min.MSE.test, 'df']
  
  # сообщение в консоль
  message(paste0('Наименьшая MSE на тестовой выборке равна ', 
                 round(min.MSE.test, 2),  
                 ' и достигается при df = ', df.min.MSE.test, '.'))

# компромисс между точностью и простотой модели по графику
df.my.MSE.test <- 6
  my.MSE.test <- tbl[tbl$df == df.my.MSE.test, 'MSE.test']
  
  # сообщение в консоль
  message(paste0('Компромисс между точностью и сложностью модели при df = ', 
                 df.my.MSE.test, ', MSE = ', round(my.MSE.test, 2), '.'))

# ставим точку на графике
abline(v = df.my.MSE.test, 
  lty = 2, lwd = 2)
points(x = df.my.MSE.test, y = my.MSE.test, 
  pch = 15, col = 'blue')
mtext(df.my.MSE.test, 
      side = 1, line = -1, at = df.my.MSE.test, col = 'blue', cex = 1.2)

#  График 3: Лучшая модель (компромисс между гибкостью и точностью) ############

mod.MSE.test <- smooth.spline(x = x.train, y = y.train, df = df.my.MSE.test)
  
  # для гладких графиков модели
  x.model.plot <- seq(x.min, x.max, length = 250)
  y.model.plot <- predict(mod.MSE.test, data.frame(x = x.model.plot))$y[, 1]

# убираем широкие поля рисунка
par(mar = c(4, 4, 1, 1))

# наименьшие/наибольшие значения по осям
x.lim <- c(x.min, x.max)
y.lim <- c(min(y), max(y))

# наблюдения с шумом (обучающая выборка)
plot(x.train, y.train, 
  col = grey(0.2), bg = grey(0.2), pch = 21,
  xlab = 'X', ylab = 'Y', 
  xlim = x.lim, ylim = y.lim, 
  cex = 1.2, cex.lab = 1.2, cex.axis = 1.2)

# наблюдения тестовой выборки
points(x.test, y.test, 
  col = 'red', bg = 'red', pch = 21)

# истинная функция
lines(x.line, y.line, 
  lwd = 2, lty = 2)

# модель
lines(x.model.plot, y.model.plot, 
  lwd = 2, col = 'blue')

# легенда
legend('topleft', legend = c('обучение', 'тест', 'f(X)', 'модель'),
       pch = c(16, 16, NA, NA), 
       col = c(grey(0.2), 'red', 'black', 'blue'),  
       lty = c(0, 0, 2, 1), lwd = c(1, 1, 2, 2), cex = 1.2)

```

# При *'res.sd'* = 2


```{r, warning=FALSE}
my.seed <- 3

# наблюдений всего
n.all <- 60
  # доля обучающей выборки
  train.percent <- 0.85
  # стандартное отклонение случайного шума
  res.sd <- 2
  # границы изменения X
  x.min <- 5
  x.max <- 105
  
  # фактические значения x
  set.seed(my.seed)
x <- runif(x.min, x.max, n = n.all)

  # случайный шум
  set.seed(my.seed)
res <- rnorm(mean = 0, sd = res.sd, n = n.all)

  # отбираем наблюдения в обучающую выборку
  set.seed(my.seed)
inTrain <-  sample(seq_along(x), size = train.percent*n.all)
  
  # истинная функция взаимосвязи 
  y.func <- function(x) {19 - 0.05 * x}

# для графика истинной взаимосвязи
x.line <- seq(x.min, x.max, length = n.all)
  y.line <- y.func(x.line)
  
  # фактические значения y (с шумом)
  y <- y.func(x) + res
  
  # Создаём векторы с данными для построения графиков ############################

# наблюдения на обучающей выборке
x.train <- x[inTrain]
  y.train <- y[inTrain]
  
  # наблюдения на тестовой выборке
  x.test <- x[-inTrain]
  y.test <- y[-inTrain]
  
  #  График 1: Исходные данные на график #########################################

# убираем широкие поля рисунка
par(mar = c(4, 4, 1, 1))

# наименьшие/наибольшие значения по осям
x.lim <- c(x.min, x.max)
y.lim <- c(min(y), max(y))

# наблюдения с шумом (обучающая выборка)
plot( x.train, y.train,
  col = grey(0.2), bg = grey(0.2), pch = 21,
  xlab = 'X', ylab = 'Y', 
  xlim = x.lim, ylim = y.lim, 
  cex = 1.2, cex.lab = 1.2, cex.axis = 1.2)

# наблюдения тестовой выборки
points(x.test, y.test,
  col = 'red', bg = 'red', pch = 21)

# истинная функция
lines(x.line, y.line,
  lwd = 2, lty = 2)

# легенда
legend('topleft', legend = c('обучение', 'тест', 'f(X)'),
       pch = c(16, 16, NA), 
       col = c(grey(0.2), 'red', 'black'),  
       lty = c(0, 0, 2), lwd = c(1, 1, 2), cex = 1.2)

#  Строим модель ###############################################################

# модель 2 (сплайн с df = 6)
mod <- smooth.spline(x = x.train, y = y.train, df = 6)

# модельные значения для расчёта ошибок
y.model.train <- predict(mod, data.frame(x = x.train))$y[, 1]
  y.model.test <- predict(mod, data.frame(x = x.train))$y[, 1]
  
  # считаем средний квадрат ошибки на обучающей и тестовой выборке
  MSE <- c(sum((y.train - y.model.train)^2) / length(x.train),
           sum((y.test - y.model.test)^2) / length(x.test))
  names(MSE) <- c('train', 'test')
  round(MSE, 2)

#  Цикл по степеням свободы ####################################################

# максимальное число степеней свободы для модели сплайна
max.df <- 40
# таблица для записи ошибок
tbl <- data.frame(df = 2:max.df)
# ошибки на обучающей выборке
tbl$MSE.train <- 0
# ошибки на тестовой выборке
tbl$MSE.test <- 0


for (i in 2:max.df) {
  # модель
  mod <-  smooth.spline(x = x.train, y = y.train, df = i)
    
    # модельные значения для расчёта ошибок
    y.model.train <- predict(mod, data.frame(x = x.train))$y[, 1]
    y.model.test <- predict(mod, data.frame(x = x.test))$y[, 1] 
    
    # считаем средний квадрат ошибки на обучающей и тестовой выборке
    MSE <- c(sum((y.train - y.model.train)^2) / length(x.train),
             sum((y.test - y.model.test)^2) / length(x.test))
    
    # записываем результат в таблицу
    tbl[tbl$df == i, c('MSE.train', 'MSE.test')] <- MSE
}

# первые строки таблицы
kable(head(tbl))

#  График 2: Зависимость MSE от гибкости модели ################################

plot(x = tbl$df, y = tbl$MSE.test, 
  type = 'l', col = 'red', lwd = 2,
  xlab = 'Степени свободы сплайна', ylab = 'MSE',
  ylim = c(min(tbl$MSE.train, tbl$MSE.test), 
           max(tbl$MSE.train, tbl$MSE.test)),
  cex = 1.2, cex.lab = 1.2, cex.axis = 1.2)

points(x = tbl$df, y = tbl$MSE.test,
  pch = 21, col = 'red', bg = 'red')

lines(x = tbl$df, y = tbl$MSE.train,
  col = grey(0.3), lwd = 2)
# неустранимая ошибка
abline(h = res.sd,
  lty = 2, col = grey(0.4), lwd = 2)

# степени свободы у наименьшей ошибки на тестовой выборке
min.MSE.test <- min(tbl$MSE.test)
  df.min.MSE.test <- tbl[tbl$MSE.test == min.MSE.test, 'df']
  
  # сообщение в консоль
  message(paste0('Наименьшая MSE на тестовой выборке равна ', 
                 round(min.MSE.test, 2),  
                 ' и достигается при df = ', df.min.MSE.test, '.'))

# компромисс между точностью и простотой модели по графику
df.my.MSE.test <- 6
  my.MSE.test <- tbl[tbl$df == df.my.MSE.test, 'MSE.test']
  
  # сообщение в консоль
  message(paste0('Компромисс между точностью и сложностью модели при df = ', 
                 df.my.MSE.test, ', MSE = ', round(my.MSE.test, 2), '.'))

# ставим точку на графике
abline(v = df.my.MSE.test, 
  lty = 2, lwd = 2)
points(x = df.my.MSE.test, y = my.MSE.test, 
  pch = 15, col = 'blue')
mtext(df.my.MSE.test, 
      side = 1, line = -1, at = df.my.MSE.test, col = 'blue', cex = 1.2)

#  График 3: Лучшая модель (компромисс между гибкостью и точностью) ############

mod.MSE.test <- smooth.spline(x = x.train, y = y.train, df = df.my.MSE.test)
  
  # для гладких графиков модели
  x.model.plot <- seq(x.min, x.max, length = 250)
  y.model.plot <- predict(mod.MSE.test, data.frame(x = x.model.plot))$y[, 1]

# убираем широкие поля рисунка
par(mar = c(4, 4, 1, 1))

# наименьшие/наибольшие значения по осям
x.lim <- c(x.min, x.max)
y.lim <- c(min(y), max(y))

# наблюдения с шумом (обучающая выборка)
plot(x.train, y.train, 
  col = grey(0.2), bg = grey(0.2), pch = 21,
  xlab = 'X', ylab = 'Y', 
  xlim = x.lim, ylim = y.lim, 
  cex = 1.2, cex.lab = 1.2, cex.axis = 1.2)

# наблюдения тестовой выборки
points(x.test, y.test, 
  col = 'red', bg = 'red', pch = 21)

# истинная функция
lines(x.line, y.line, 
  lwd = 2, lty = 2)

# модель
lines(x.model.plot, y.model.plot, 
  lwd = 2, col = 'blue')

# легенда
legend('topleft', legend = c('обучение', 'тест', 'f(X)', 'модель'),
       pch = c(16, 16, NA, NA), 
       col = c(grey(0.2), 'red', 'black', 'blue'),  
       lty = c(0, 0, 2, 1), lwd = c(1, 1, 2, 2), cex = 1.2)

```

# При *'res.sd'* = 3


```{r, warning=FALSE}
my.seed <- 3

# наблюдений всего
n.all <- 60
  # доля обучающей выборки
  train.percent <- 0.85
  # стандартное отклонение случайного шума
  res.sd <- 3
  # границы изменения X
  x.min <- 5
  x.max <- 105
  
  # фактические значения x
  set.seed(my.seed)
x <- runif(x.min, x.max, n = n.all)

  # случайный шум
  set.seed(my.seed)
res <- rnorm(mean = 0, sd = res.sd, n = n.all)

  # отбираем наблюдения в обучающую выборку
  set.seed(my.seed)
inTrain <-  sample(seq_along(x), size = train.percent*n.all)
  
  # истинная функция взаимосвязи 
  y.func <- function(x) {19 - 0.05 * x}

# для графика истинной взаимосвязи
x.line <- seq(x.min, x.max, length = n.all)
  y.line <- y.func(x.line)
  
  # фактические значения y (с шумом)
  y <- y.func(x) + res
  
  # Создаём векторы с данными для построения графиков ############################

# наблюдения на обучающей выборке
x.train <- x[inTrain]
  y.train <- y[inTrain]
  
  # наблюдения на тестовой выборке
  x.test <- x[-inTrain]
  y.test <- y[-inTrain]
  
  #  График 1: Исходные данные на график #########################################

# убираем широкие поля рисунка
par(mar = c(4, 4, 1, 1))

# наименьшие/наибольшие значения по осям
x.lim <- c(x.min, x.max)
y.lim <- c(min(y), max(y))

# наблюдения с шумом (обучающая выборка)
plot( x.train, y.train,
  col = grey(0.2), bg = grey(0.2), pch = 21,
  xlab = 'X', ylab = 'Y', 
  xlim = x.lim, ylim = y.lim, 
  cex = 1.2, cex.lab = 1.2, cex.axis = 1.2)

# наблюдения тестовой выборки
points(x.test, y.test,
  col = 'red', bg = 'red', pch = 21)

# истинная функция
lines(x.line, y.line,
  lwd = 2, lty = 2)

# легенда
legend('topleft', legend = c('обучение', 'тест', 'f(X)'),
       pch = c(16, 16, NA), 
       col = c(grey(0.2), 'red', 'black'),  
       lty = c(0, 0, 2), lwd = c(1, 1, 2), cex = 1.2)

#  Строим модель ###############################################################

# модель 2 (сплайн с df = 6)
mod <- smooth.spline(x = x.train, y = y.train, df = 6)

# модельные значения для расчёта ошибок
y.model.train <- predict(mod, data.frame(x = x.train))$y[, 1]
  y.model.test <- predict(mod, data.frame(x = x.train))$y[, 1]
  
  # считаем средний квадрат ошибки на обучающей и тестовой выборке
  MSE <- c(sum((y.train - y.model.train)^2) / length(x.train),
           sum((y.test - y.model.test)^2) / length(x.test))
  names(MSE) <- c('train', 'test')
  round(MSE, 2)

#  Цикл по степеням свободы ####################################################

# максимальное число степеней свободы для модели сплайна
max.df <- 40
# таблица для записи ошибок
tbl <- data.frame(df = 2:max.df)
# ошибки на обучающей выборке
tbl$MSE.train <- 0
# ошибки на тестовой выборке
tbl$MSE.test <- 0

for (i in 2:max.df) {
  # модель
  mod <-  smooth.spline(x = x.train, y = y.train, df = i)
    
    # модельные значения для расчёта ошибок
    y.model.train <- predict(mod, data.frame(x = x.train))$y[, 1]
    y.model.test <- predict(mod, data.frame(x = x.test))$y[, 1] 
    
    # считаем средний квадрат ошибки на обучающей и тестовой выборке
    MSE <- c(sum((y.train - y.model.train)^2) / length(x.train),
             sum((y.test - y.model.test)^2) / length(x.test))
    
    # записываем результат в таблицу
    tbl[tbl$df == i, c('MSE.train', 'MSE.test')] <- MSE
}

# первые строки таблицы
kable(head(tbl))

#  График 2: Зависимость MSE от гибкости модели ################################

plot(x = tbl$df, y = tbl$MSE.test, 
  type = 'l', col = 'red', lwd = 2,
  xlab = 'Степени свободы сплайна', ylab = 'MSE',
  ylim = c(min(tbl$MSE.train, tbl$MSE.test), 
           max(tbl$MSE.train, tbl$MSE.test)),
  cex = 1.2, cex.lab = 1.2, cex.axis = 1.2)

points(x = tbl$df, y = tbl$MSE.test,
  pch = 21, col = 'red', bg = 'red')

lines(x = tbl$df, y = tbl$MSE.train,
  col = grey(0.3), lwd = 2)
# неустранимая ошибка
abline(h = res.sd,
  lty = 2, col = grey(0.4), lwd = 2)

# степени свободы у наименьшей ошибки на тестовой выборке
min.MSE.test <- min(tbl$MSE.test)
  df.min.MSE.test <- tbl[tbl$MSE.test == min.MSE.test, 'df']
  
  # сообщение в консоль
  message(paste0('Наименьшая MSE на тестовой выборке равна ', 
                 round(min.MSE.test, 2),  
                 ' и достигается при df = ', df.min.MSE.test, '.'))

# компромисс между точностью и простотой модели по графику
df.my.MSE.test <- 6
  my.MSE.test <- tbl[tbl$df == df.my.MSE.test, 'MSE.test']
  
  # сообщение в консоль
  message(paste0('Компромисс между точностью и сложностью модели при df = ', 
                 df.my.MSE.test, ', MSE = ', round(my.MSE.test, 2), '.'))

# ставим точку на графике
abline(v = df.my.MSE.test, 
  lty = 2, lwd = 2)
points(x = df.my.MSE.test, y = my.MSE.test, 
  pch = 15, col = 'blue')
mtext(df.my.MSE.test, 
      side = 1, line = -1, at = df.my.MSE.test, col = 'blue', cex = 1.2)

#  График 3: Лучшая модель (компромисс между гибкостью и точностью) ############

mod.MSE.test <- smooth.spline(x = x.train, y = y.train, df = df.my.MSE.test)
  
  # для гладких графиков модели
  x.model.plot <- seq(x.min, x.max, length = 250)
  y.model.plot <- predict(mod.MSE.test, data.frame(x = x.model.plot))$y[, 1]

# убираем широкие поля рисунка
par(mar = c(4, 4, 1, 1))

# наименьшие/наибольшие значения по осям
x.lim <- c(x.min, x.max)
y.lim <- c(min(y), max(y))

# наблюдения с шумом (обучающая выборка)
plot(x.train, y.train, 
  col = grey(0.2), bg = grey(0.2), pch = 21,
  xlab = 'X', ylab = 'Y', 
  xlim = x.lim, ylim = y.lim, 
  cex = 1.2, cex.lab = 1.2, cex.axis = 1.2)

# наблюдения тестовой выборки
points(x.test, y.test, 
  col = 'red', bg = 'red', pch = 21)

# истинная функция
lines(x.line, y.line, 
  lwd = 2, lty = 2)

# модель
lines(x.model.plot, y.model.plot, 
  lwd = 2, col = 'blue')

# легенда
legend('topleft', legend = c('обучение', 'тест', 'f(X)', 'модель'),
       pch = c(16, 16, NA, NA), 
       col = c(grey(0.2), 'red', 'black', 'blue'),  
       lty = c(0, 0, 2, 1), lwd = c(1, 1, 2, 2), cex = 1.2)

```

# При *'res.sd'* = 4


```{r, warning=FALSE}
my.seed <- 3

# наблюдений всего
n.all <- 60
  # доля обучающей выборки
  train.percent <- 0.85
  # стандартное отклонение случайного шума
  res.sd <- 4
  # границы изменения X
  x.min <- 5
  x.max <- 105
  
  # фактические значения x
  set.seed(my.seed)
x <- runif(x.min, x.max, n = n.all)

  # случайный шум
  set.seed(my.seed)
res <- rnorm(mean = 0, sd = res.sd, n = n.all)

  # отбираем наблюдения в обучающую выборку
  set.seed(my.seed)
inTrain <-  sample(seq_along(x), size = train.percent*n.all)
  
  # истинная функция взаимосвязи 
  y.func <- function(x) {19 - 0.05 * x}

# для графика истинной взаимосвязи
x.line <- seq(x.min, x.max, length = n.all)
  y.line <- y.func(x.line)
  
  # фактические значения y (с шумом)
  y <- y.func(x) + res
  
  # Создаём векторы с данными для построения графиков ############################

# наблюдения на обучающей выборке
x.train <- x[inTrain]
  y.train <- y[inTrain]
  
  # наблюдения на тестовой выборке
  x.test <- x[-inTrain]
  y.test <- y[-inTrain]
  
  #  График 1: Исходные данные на график #########################################

# убираем широкие поля рисунка
par(mar = c(4, 4, 1, 1))

# наименьшие/наибольшие значения по осям
x.lim <- c(x.min, x.max)
y.lim <- c(min(y), max(y))

# наблюдения с шумом (обучающая выборка)
plot( x.train, y.train,
  col = grey(0.2), bg = grey(0.2), pch = 21,
  xlab = 'X', ylab = 'Y', 
  xlim = x.lim, ylim = y.lim, 
  cex = 1.2, cex.lab = 1.2, cex.axis = 1.2)

# наблюдения тестовой выборки
points(x.test, y.test,
  col = 'red', bg = 'red', pch = 21)

# истинная функция
lines(x.line, y.line,
  lwd = 2, lty = 2)

# легенда
legend('topleft', legend = c('обучение', 'тест', 'f(X)'),
       pch = c(16, 16, NA), 
       col = c(grey(0.2), 'red', 'black'),  
       lty = c(0, 0, 2), lwd = c(1, 1, 2), cex = 1.2)

#  Строим модель ###############################################################

# модель 2 (сплайн с df = 6)
mod <- smooth.spline(x = x.train, y = y.train, df = 6)

# модельные значения для расчёта ошибок
y.model.train <- predict(mod, data.frame(x = x.train))$y[, 1]
  y.model.test <- predict(mod, data.frame(x = x.train))$y[, 1]
  
  # считаем средний квадрат ошибки на обучающей и тестовой выборке
  MSE <- c(sum((y.train - y.model.train)^2) / length(x.train),
           sum((y.test - y.model.test)^2) / length(x.test))
  names(MSE) <- c('train', 'test')
  round(MSE, 2)

#  Цикл по степеням свободы ####################################################

# максимальное число степеней свободы для модели сплайна
max.df <- 40
# таблица для записи ошибок
tbl <- data.frame(df = 2:max.df)
# ошибки на обучающей выборке
tbl$MSE.train <- 0
# ошибки на тестовой выборке
tbl$MSE.test <- 0


for (i in 2:max.df) {
  # модель
  mod <-  smooth.spline(x = x.train, y = y.train, df = i)
    
    # модельные значения для расчёта ошибок
    y.model.train <- predict(mod, data.frame(x = x.train))$y[, 1]
    y.model.test <- predict(mod, data.frame(x = x.test))$y[, 1] 
    
    # считаем средний квадрат ошибки на обучающей и тестовой выборке
    MSE <- c(sum((y.train - y.model.train)^2) / length(x.train),
             sum((y.test - y.model.test)^2) / length(x.test))
    
    # записываем результат в таблицу
    tbl[tbl$df == i, c('MSE.train', 'MSE.test')] <- MSE
}

# первые строки таблицы
kable(head(tbl))

#  График 2: Зависимость MSE от гибкости модели ################################

plot(x = tbl$df, y = tbl$MSE.test, 
  type = 'l', col = 'red', lwd = 2,
  xlab = 'Степени свободы сплайна', ylab = 'MSE',
  ylim = c(min(tbl$MSE.train, tbl$MSE.test), 
           max(tbl$MSE.train, tbl$MSE.test)),
  cex = 1.2, cex.lab = 1.2, cex.axis = 1.2)

points(x = tbl$df, y = tbl$MSE.test,
  pch = 21, col = 'red', bg = 'red')

lines(x = tbl$df, y = tbl$MSE.train,
  col = grey(0.3), lwd = 2)
# неустранимая ошибка
abline(h = res.sd,
  lty = 2, col = grey(0.4), lwd = 2)

# степени свободы у наименьшей ошибки на тестовой выборке
min.MSE.test <- min(tbl$MSE.test)
  df.min.MSE.test <- tbl[tbl$MSE.test == min.MSE.test, 'df']
  
  # сообщение в консоль
  message(paste0('Наименьшая MSE на тестовой выборке равна ', 
                 round(min.MSE.test, 2),  
                 ' и достигается при df = ', df.min.MSE.test, '.'))

# компромисс между точностью и простотой модели по графику
df.my.MSE.test <- 6
  my.MSE.test <- tbl[tbl$df == df.my.MSE.test, 'MSE.test']
  
  # сообщение в консоль
  message(paste0('Компромисс между точностью и сложностью модели при df = ', 
                 df.my.MSE.test, ', MSE = ', round(my.MSE.test, 2), '.'))

# ставим точку на графике
abline(v = df.my.MSE.test, 
  lty = 2, lwd = 2)
points(x = df.my.MSE.test, y = my.MSE.test, 
  pch = 15, col = 'blue')
mtext(df.my.MSE.test, 
      side = 1, line = -1, at = df.my.MSE.test, col = 'blue', cex = 1.2)

#  График 3: Лучшая модель (компромисс между гибкостью и точностью) ############

mod.MSE.test <- smooth.spline(x = x.train, y = y.train, df = df.my.MSE.test)
  
  # для гладких графиков модели
  x.model.plot <- seq(x.min, x.max, length = 250)
  y.model.plot <- predict(mod.MSE.test, data.frame(x = x.model.plot))$y[, 1]

# убираем широкие поля рисунка
par(mar = c(4, 4, 1, 1))

# наименьшие/наибольшие значения по осям
x.lim <- c(x.min, x.max)
y.lim <- c(min(y), max(y))

# наблюдения с шумом (обучающая выборка)
plot(x.train, y.train, 
  col = grey(0.2), bg = grey(0.2), pch = 21,
  xlab = 'X', ylab = 'Y', 
  xlim = x.lim, ylim = y.lim, 
  cex = 1.2, cex.lab = 1.2, cex.axis = 1.2)

# наблюдения тестовой выборки
points(x.test, y.test, 
  col = 'red', bg = 'red', pch = 21)

# истинная функция
lines(x.line, y.line, 
  lwd = 2, lty = 2)

# модель
lines(x.model.plot, y.model.plot, 
  lwd = 2, col = 'blue')

# легенда
legend('topleft', legend = c('обучение', 'тест', 'f(X)', 'модель'),
       pch = c(16, 16, NA, NA), 
       col = c(grey(0.2), 'red', 'black', 'blue'),  
       lty = c(0, 0, 2, 1), lwd = c(1, 1, 2, 2), cex = 1.2)

```

Среднее квадратическое отклонение (MSE) - мера рассеяния значений случайной величины относиетльно её среднего значения (математического ожидания) - корень квадратный из дисперсии случайной величины (второго центрального момента распределения). А стандартное отклонение *res.sd* - синоним квадратического отклонения. Оно вычисляется как корень квадратный из суммы квадратов отклонений отсчётов (значений случайной величины) от некоторой постоянной величины (обычно среднего арифметического всех отсётов), разделённой на число взятых отсчётов. Поэтому при увеличении значения *res.sd* пропорционально увеличивается значение MSE.